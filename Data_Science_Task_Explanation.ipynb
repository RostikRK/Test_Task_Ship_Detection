{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Test task - Airbus ship detection\n",
    "\n",
    "### By Rostyslav Kostiuk"
   ],
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "id": "519be69f-b7e5-4f00-a56d-c72bc803892d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-coding part\n",
    "\n",
    "So before I started this task, I had to prepare myself a little bit because I had never dealt with image segmentation before. I started from the concept of convolutional neural networks. In order to remind myself how they work, I watched a video on these networks from MIT. There I noticed an interesting architecture of networks that are used for this kind of problem - U-net. And in order to understand how it works, I looked at a whole playlist on this topic and also read a couple of articles. So I was almost ready but had to deal with the nuances of this challenge and so I watched a few more videos in which people talked about the experience of participating in this challenge (no code).\n",
    "\n",
    "In the end, given the fact that I am still passing a session, I completed the analysis of the theory on Friday and there are 3 days left for the coding and training itself."
   ],
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "id": "0deae7b1-6df5-4b09-974e-4ddc35658053"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparation\n",
    "Before running the key blocks lets install and import some libraries that I used for the solution"
   ],
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "id": "7b711583-5dd8-40d6-a12a-598a7e8c4330"
  },
  {
   "cell_type": "code",
   "source": [
    "!pip uninstall -y tensorflow-io\n",
    "!pip install -U -q segmentation_models"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "id": "0f40a600-7cb2-44c1-8f04-20e3e4328f2e"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from albumentations import HorizontalFlip, VerticalFlip, Compose, RandomBrightnessContrast,ShiftScaleRotate, GaussNoise\n",
    "import os\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import segmentation_models as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "from sklearn.metrics import jaccard_score"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "id": "bb4f671a-3e75-4308-8195-5330c677a001"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Key idea\n",
    "The key idea behind this task is to develop a two-step solution for the Airbus Ship Detection Challenge. The first step involves creating a classification model to determine whether or not an image contains a ship. The second step involves creating a segmentation model to identify the exact location of the ship in the images that were classified as containing a ship. The classification model helps to reduce the computational load for the segmentation model by filtering out images without ships."
   ],
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "id": "09149d31-6d03-463d-8589-8d25cea4c2db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification model - training data\n",
    "The first step in training a classification model is preparing the data. In this task, we have a set of images, each of which may or may not contain a ship. The images are labeled accordingly, providing us with a supervised learning problem.\n",
    "\n",
    "Firstly I have prepared a functions that will help to crop the image into the patches 256 x 256 and another one that helps to preprocess our run length encoding into  the binary mask"
   ],
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "id": "3f74841e-118d-4a4c-9333-730dae0c5ae3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def crop_image(image, crop_size=256):\n",
    "    if isinstance(image, str):  # If the image is a file path\n",
    "        img = Image.open(image)\n",
    "    elif isinstance(image, np.ndarray):  # If the image is a numpy array\n",
    "        img = Image.fromarray(image)\n",
    "\n",
    "    width, height = img.size\n",
    "\n",
    "    crops = []\n",
    "    for i in range(0, height, crop_size):\n",
    "        for j in range(0, width, crop_size):\n",
    "            box = (j, i, j + crop_size, i + crop_size)\n",
    "            crop = img.crop(box)\n",
    "            crops.append(np.array(crop))\n",
    "\n",
    "    return crops\n",
    "\n",
    "\n",
    "def rle_to_mask(rle_list, shape=(768, 768)):\n",
    "    # Convert the run-length encoding to a binary mask\n",
    "    mask = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "\n",
    "    for rle in rle_list:\n",
    "        if pd.isnull(rle):  # If the RLE is NaN, skip this loop iteration\n",
    "            continue\n",
    "        starts, lengths = map(np.asarray, (rle.split()[0:][::2], rle.split()[1:][::2]))\n",
    "        starts = starts.astype(int) - 1\n",
    "        lengths = lengths.astype(int)  # Convert lengths to int\n",
    "        ends = starts + lengths\n",
    "        for start, end in zip(starts, ends):\n",
    "            mask[start:end] = 1\n",
    "\n",
    "    return mask.reshape(shape).T  # Reshape the mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we just read the data segmentation csv, so change the path for a fie"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('path_to_file.csv')\n",
    "\n",
    "# Group the DataFrame by 'ImageId' to get a list of masks for each image\n",
    "grouped = df.groupby('ImageId')['EncodedPixels'].apply(list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And as a last step we create a patch images and responding for them labels. We take the image crop it and the respencive mask. Then if there in binary mask exist a pixel that classifies a ship we label this image with 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize empty lists to store the image patches and labels\n",
    "patch_images = []\n",
    "labels = []\n",
    "\n",
    "# Loop over the grouped DataFrame\n",
    "for filename, rle_list in grouped[:int(len(grouped)*0.012)].items():\n",
    "    image_path = 'path_to_image_folder' + filename\n",
    "    # Crop the image into patches\n",
    "    crops = crop_image(image_path)\n",
    "    # Convert the run-length encoding to a binary mask\n",
    "    mask = rle_to_mask(rle_list)\n",
    "    # Crop the mask into patches\n",
    "    mask_crops = crop_image(mask)\n",
    "    # Loop over the image and mask patches\n",
    "    for img, msk in zip(crops, mask_crops):\n",
    "        # Label the patch as '1' if it contains a ship (mask is not all zeros), '0' otherwise\n",
    "        label = 1 if np.any(msk) else 0\n",
    "        # Append the image patch and its label to their respective lists\n",
    "        patch_images.append(img)\n",
    "        labels.append(label)\n",
    "\n",
    "# Convert the lists into numpy arrays for future use\n",
    "patch_images = np.array(patch_images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(patch_images, labels, test_size=0.2, random_state=42)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification model - training model\n",
    "For the classification task, we chose to use a MobileNet model. MobileNet is a type of convolutional neural network designed for mobile and embedded vision applications. It's known for being lightweight and efficient, with lower computational requirements than many other models, making it a good choice for tasks where computational resources may be limited.\n",
    "\n",
    "MobileNet achieves this efficiency through the use of depthwise separable convolutions, a type of convolution that reduces the number of parameters and computations in the network. This makes the network faster and smaller, while still maintaining a high level of performance.\n",
    "\n",
    "In our case, we use MobileNet as a feature extractor, and add a few additional layers on top to perform the binary classification task (ship or no ship). The model is trained using a binary cross-entropy loss function, which is suitable for binary classification problems."
   ],
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "id": "3aa64530-00a5-44a5-8b90-d27f5a2cfa0b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "set_global_policy('mixed_float16')\n",
    "\n",
    "# Define a data generator for on-the-fly normalization\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Use the data generator to load the data\n",
    "train_generator = datagen.flow(X_train, y_train, batch_size=32)\n",
    "val_generator = datagen.flow(X_val, y_val, batch_size=32)\n",
    "\n",
    "# Load the pre-trained MobileNetV2 model, excluding the top layer\n",
    "base_model = MobileNet(weights='imagenet', include_top=False)\n",
    "\n",
    "# Add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a fully-connected layer\n",
    "x = Dense(512, activation='relu')(x)\n",
    "\n",
    "# Add a logistic layer for binary classification\n",
    "# Change the policy for the output layer to 'float32' for stability\n",
    "predictions = Dense(1, activation='sigmoid', dtype='float32')(x)\n",
    "\n",
    "# Construct the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set up the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Set up the model checkpoint callback to save the best model based on validation accuracy\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "model.fit(train_generator, validation_data=val_generator, steps_per_epoch=len(X_train) // 32,\n",
    "          epochs=20, callbacks=[early_stopping, model_checkpoint])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification model - testing model\n",
    "To test the model's performance I decide to  perform just a 'sanity check' to ensure the model's predictions make sense. This involves visually inspecting some of the images along with their predicted labels and the model's predicted probabilities. This can help us catch any obvious errors or issues with the model's predictions.\n",
    "\n"
   ],
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "id": "a16e3223-c031-46a7-87c1-53b5d9f9e039"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_model(X_test, y_test):\n",
    "    # Randomly select an index from the test data\n",
    "    idx = random.randint(0, len(X_test) - 1)\n",
    "\n",
    "    # Get the corresponding image and label\n",
    "    image = X_test[idx]\n",
    "    label = y_test[idx]\n",
    "\n",
    "    # Normalize the image\n",
    "    image = image / 255.0\n",
    "\n",
    "    # Convert the image to an array and expand dimensions for model prediction\n",
    "    image_array = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(image_array)\n",
    "\n",
    "    # Interpret the model's prediction\n",
    "    if prediction > 0.5:\n",
    "        prediction_text = \"Ship Detected\"\n",
    "    else:\n",
    "        prediction_text = \"No Ship Detected\"\n",
    "\n",
    "    # Display the image and the model's prediction\n",
    "    plt.imshow(X_test[idx])\n",
    "    plt.title(f\"Actual: {'Ship Detected' if label == 1 else 'No Ship Detected'}\\nPredicted: {prediction_text}\")\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_model(X_val, y_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Segmentation model - training data\n",
    "The training data for the segmentation model is prepared in a similar way to the classification model, with a few key differences. The main difference is that instead of binary labels indicating the presence or absence of a ship, we have pixel-level labels indicating the exact location of the ship in the image. These labels are typically in the form of a binary mask, with 1s where the ship is located and 0s elsewhere.\n",
    "\n",
    "Another key aspect of preparing the data for the segmentation model is the use of data augmentation. Data augmentation involves creating modified versions of the images in the training set, which can help the model generalize better to new data. For this task, we use a variety of augmentation techniques, including horizontal and vertical flips, random brightness and contrast adjustments, shift and scale transformations, and the addition of Gaussian noise. These augmentations can help the model learn to recognize ships in a variety of different conditions and orientations.\n",
    "\n",
    "As with the classification model, the data is then split into a training set and a validation set. The training set is used to train the model, while the validation set is used to evaluate the model's performance and tune hyperparameters."
   ],
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "id": "a803b6d5-d5f9-4799-b22a-bea9dc2edd0a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to augment images and masks\n",
    "def augment_image(image, mask):\n",
    "    transform = Compose([\n",
    "        HorizontalFlip(p=0.4),\n",
    "        VerticalFlip(p=0.4),\n",
    "        RandomBrightnessContrast(p=0.2),\n",
    "        ShiftScaleRotate(p=0.1),\n",
    "        GaussNoise(p=0.2)\n",
    "    ])\n",
    "    data = {\"image\": np.array(image), \"mask\": mask}\n",
    "    augmented = transform(**data)\n",
    "\n",
    "    return augmented[\"image\"], augmented[\"mask\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aug_images = []\n",
    "aug_masks = []\n",
    "\n",
    "# Loop over the grouped DataFrame\n",
    "for filename, rle_list in grouped.items():\n",
    "    image_path = 'path_to_folder_with_photo' + filename\n",
    "    # Crop the image into patches\n",
    "    crops = crop_image(image_path)\n",
    "    # Convert the run-length encoding to a binary mask\n",
    "    mask = rle_to_mask(rle_list)\n",
    "    # Crop the mask into patches\n",
    "    mask_crops = crop_image(mask)\n",
    "    # Loop over the image and mask patches\n",
    "    for img, msk in zip(crops, mask_crops):\n",
    "        # Normalize the image patch\n",
    "        img_norm = img / 255.0\n",
    "        # Reshape the image patch for prediction\n",
    "        img_pred = np.expand_dims(img_norm, axis=0)\n",
    "        # Predict if the image patch contains a ship\n",
    "        prediction = model.predict(img_pred, verbose=0)\n",
    "        if prediction >= 0.5:  # If the model predicts a ship\n",
    "            for _ in range(3):\n",
    "                aug_img, aug_msk = augment_image(img, msk)\n",
    "                # Append the augmented image and mask patches to their respective lists\n",
    "                aug_images.append(aug_img)\n",
    "                aug_masks.append(aug_msk)\n",
    "\n",
    "# Convert the lists into numpy arrays for future use\n",
    "aug_images = np.array(aug_images)\n",
    "aug_masks = np.array(aug_masks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And again a small sanity check"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(aug_images))\n",
    "\n",
    "# Display the image patch\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(aug_images[idx])\n",
    "plt.title('Image Patch')\n",
    "\n",
    "# Display the corresponding mask\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(aug_masks[idx], cmap='gray')\n",
    "plt.title('Mask Patch')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Segmentation model - training model (Resnet18)\n",
    "For the segmentation task, we chose to use a U-Net architecture with a ResNet18 backbone. The U-Net architecture is particularly suited for image segmentation tasks because it combines the strengths of a contracting path (for context) and an expansive path (for localization).\n",
    "\n",
    "The ResNet18 backbone is used to extract features from the images. ResNet, or Residual Network, is a type of convolutional neural network that uses skip connections or shortcuts to jump over some layers. This helps to solve the vanishing gradient problem, allowing the network to be deeper and therefore able to learn more complex features.\n",
    "\n",
    "The choice of U-Net with a ResNet18 backbone was driven by a few factors. First, U-Net has proven to be very effective for image segmentation tasks, making it a natural choice for this task. Second, ResNet18 is relatively lightweight and efficient, making it feasible to train even with limited hardware resources. Finally, the combination of U-Net and ResNet18 is known to be effective and has been used successfully in many similar tasks.\n",
    "\n",
    "The model is trained using a suitable loss function for segmentation tasks, such as binary cross-entropy or Dice loss. During training, we monitor the model's performance on the validation set to avoid overfitting and to tune hyperparameters."
   ],
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "id": "c32a9056-1cbe-4997-acb1-6b62802ea4df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BACKBONE = 'resnet18'  # Use ResNet18 as the backbone\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "\n",
    "# Split your data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(aug_images, aug_masks, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess input\n",
    "x_train = preprocess_input(x_train)\n",
    "x_val = preprocess_input(x_val)\n",
    "y_train = y_train.astype('float32')\n",
    "y_val = y_val.astype('float32')  # replace with your validation masks\n",
    "\n",
    "\n",
    "# Define model\n",
    "model = sm.Unet(BACKBONE, encoder_weights='imagenet')\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss=sm.losses.DiceLoss(),\n",
    "    metrics=[sm.metrics.IOUScore()],\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"best_model_segment.h5\", monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, early_stopping]\n",
    "\n",
    "# Fit model\n",
    "model.fit(\n",
    "   x=x_train,\n",
    "   y=y_train,\n",
    "   batch_size=16,\n",
    "   epochs=100,\n",
    "   validation_data=(x_val, y_val),\n",
    "   callbacks=callbacks_list\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model evaluations or its inference (Resnet18)\n",
    "After training the segmentation model, we evaluate its performance using two common metrics for segmentation tasks: the Dice coefficient and the Intersection over Union (IoU) score.\n",
    "\n",
    "But firstly let`s prepare the testing data as in the kaggle dataset there is no adequately prepared one soI decide to use the part of training that I have never used before\n"
   ],
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "id": "9775cb91-18b2-414f-8b38-3b18e548fac9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get a list of unique image IDs\n",
    "image_ids = grouped.index.tolist()\n",
    "# Calculate the 90% mark\n",
    "split_index = int(len(image_ids) * 0.90)\n",
    "# Get the test image IDs\n",
    "test_image_ids = image_ids[split_index:]\n",
    "\n",
    "\n",
    "# Path to the images\n",
    "image_dir = \"path_to_images\"\n",
    "\n",
    "# Initialize variables to store the true and predicted masks\n",
    "true_masks = []\n",
    "pred_masks = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over the test image IDs\n",
    "for image_id in test_image_ids:\n",
    "    # Load the image\n",
    "    image = cv2.imread(os.path.join(image_dir, image_id))\n",
    "\n",
    "    # Preprocess the image\n",
    "    image = cv2.resize(image, (256, 256))  # resize to match the model's expected input size\n",
    "    image = preprocess_input(image)\n",
    "\n",
    "    # Reshape the image\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # Make the prediction\n",
    "    prediction = model.predict(image, verbose=0)[0]\n",
    "\n",
    "    # Add the predicted mask to the list\n",
    "    pred_masks.append(prediction)\n",
    "\n",
    "    # Get the true mask for this image\n",
    "    rle_encoded_mask = grouped.loc[image_id]\n",
    "\n",
    "    # Decode the RLE-encoded mask\n",
    "    true_mask = rle_to_mask(rle_encoded_mask, (768, 768))  # replace with your function to decode the mask\n",
    "    true_mask = cv2.resize(true_mask, (256, 256))\n",
    "\n",
    "    # Add the true mask to the list\n",
    "    true_masks.append(true_mask)\n",
    "\n",
    "# convert to a binary mask\n",
    "binary_pred_masks = np.where(np.concatenate(pred_masks) > 0.5, 1, 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Dice Coefficient:**  For image segmentation tasks, it can be used to measure the similarity between the predicted segmentation and the ground truth. The Dice coefficient ranges from 0 to 1, where 1 indicates perfect agreement between the two samples, and 0 indicates no agreement. Its so called f1 score in terms of area.\n",
    "\n",
    "**Intersection over Union (IoU):** Also known as the Jaccard index, this is another statistical tool used to measure the similarity between two samples. It is defined as the size of the intersection divided by the size of the union of two label sets. For image segmentation tasks, it can be used to measure the overlap between the predicted segmentation and the ground truth."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "iou_score = jaccard_score(np.concatenate(true_masks).ravel(), binary_pred_masks.ravel())\n",
    "\n",
    "print(f\"IOU Score: {iou_score}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dice_score(y_true, y_pred):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + 1.) / (np.sum(y_true_f) + np.sum(y_pred_f) + 1.)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dice = dice_score(np.concatenate(true_masks),  binary_pred_masks.ravel())\n",
    "\n",
    "print(f\"Dice Score: {dice}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The evaluation of the resnet gave us such results (you can check by yourself):\n",
    "\n",
    "- IOU Score - 12%\n",
    "- Dice Score - 5%\n",
    "\n",
    "These are a pretty bad result so lets perform again a small check and look where is an issue"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select a random image ID\n",
    "random_id = np.random.choice(test_image_ids)\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(os.path.join(image_dir, random_id))\n",
    "\n",
    "# Preprocess the image\n",
    "preprocessed_image = cv2.resize(image, (256, 256))  # resize to match the model's expected input size\n",
    "preprocessed_image = preprocess_input(preprocessed_image)\n",
    "\n",
    "# Reshape the image\n",
    "preprocessed_image = np.expand_dims(preprocessed_image, axis=0)\n",
    "\n",
    "# Make the prediction\n",
    "prediction = model.predict(preprocessed_image)[0]\n",
    "\n",
    "# Get the true mask for this image\n",
    "rle_encoded_mask = grouped.loc[random_id]\n",
    "\n",
    "# Decode the RLE-encoded mask\n",
    "true_mask = rle_to_mask(rle_encoded_mask, (768, 768))  # replace with your function to decode the mask\n",
    "true_mask = cv2.resize(true_mask, (256, 256))\n",
    "\n",
    "# Plot the image, the actual mask, and the predicted mask\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 20))\n",
    "\n",
    "axs[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "axs[0].set_title('Image')\n",
    "\n",
    "axs[1].imshow(true_mask, cmap='gray')\n",
    "axs[1].set_title('Actual Mask')\n",
    "\n",
    "axs[2].imshow(prediction.squeeze(), cmap='gray')\n",
    "axs[2].set_title('Predicted Mask')\n",
    "\n",
    "for ax in axs:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see there are a lot of problems: in some places it don`t see a ship (rarely), in a lot of situation it predicts the ship where it is not actually, and also the form of prediction is like spot not a rectangle."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Segmentation model - training model (EfficientNetB4)\n",
    "After evaluating the performance of the ResNet18 model, I decided to train a second model using the EfficientNetB4 architecture. EfficientNet is a family of models that were designed to achieve good performance while being efficient in terms of computational resources.\n",
    "\n",
    "The key idea behind EfficientNet is to scale up the width, depth, and resolution of the model in a balanced way. Traditional approaches to improving the performance of convolutional neural networks often involve increasing the depth (adding more layers) or the width (adding more channels) of the model. However, these approaches can lead to increased computational requirements and may not always lead to improved performance. EfficientNet addresses this by using a compound scaling method that scales up all dimensions of the model in a balanced way.\n",
    "\n",
    "The choice of EfficientNetB4 was driven by its balance of performance and efficiency, as well as its success in similar tasks."
   ],
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "id": "24f4803b-eaa4-4de7-b5f7-d1122b637bf5"
  },
  {
   "cell_type": "code",
   "source": [
    "BACKBONE = 'efficientnetb4'  # Use EfficientNetB4 as the backbone\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "\n",
    "# Split your data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(aug_images, aug_masks, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess input\n",
    "x_train = preprocess_input(x_train)\n",
    "x_val = preprocess_input(x_val)\n",
    "y_train = y_train.astype('float32')\n",
    "y_val = y_val.astype('float32')\n",
    "\n",
    "# Define model\n",
    "model = sm.Unet(BACKBONE, encoder_weights='imagenet')\n",
    "model.compile(\n",
    "    optimizer=Adam(lr=1e-4),  # Decrease learning rate\n",
    "    loss=sm.losses.binary_crossentropy + sm.losses.dice_loss,  # Use combination of BCE and Dice loss\n",
    "    metrics=[sm.metrics.IOUScore()],\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"best_model_segment.h5\", monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1)  # Increase patience\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)  # Add ReduceLROnPlateau callback\n",
    "\n",
    "callbacks_list = [checkpoint, early_stopping, reduce_lr]\n",
    "\n",
    "# Fit model\n",
    "model.fit(\n",
    "   x=x_train,\n",
    "   y=y_train,\n",
    "   batch_size=16,\n",
    "   epochs=100,\n",
    "   validation_data=(x_val, y_val),\n",
    "   callbacks=callbacks_list\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "id": "a6cc4f43-46bc-4b86-b2a0-c452cc7f2b00"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model evaluations or its inference (EfficientNetB4)\n",
    "\n",
    "To exaluate the performance of these model I used the same approach as for the ResNet18"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize variables to store the true and predicted masks\n",
    "true_masks = []\n",
    "pred_masks = []\n",
    "\n",
    "# Iterate over the test image IDs\n",
    "for image_id in test_image_ids:\n",
    "    # Load the image\n",
    "    image = cv2.imread(os.path.join(image_dir, image_id))\n",
    "\n",
    "    # Preprocess the image\n",
    "    image = cv2.resize(image, (256, 256))  # resize to match the model's expected input size\n",
    "    image = preprocess_input(image)\n",
    "\n",
    "    # Reshape the image\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # Make the prediction\n",
    "    prediction = model.predict(image, verbose=0)[0]\n",
    "\n",
    "    # Add the predicted mask to the list\n",
    "    pred_masks.append(prediction)\n",
    "\n",
    "    # Get the true mask for this image\n",
    "    rle_encoded_mask = grouped.loc[image_id]\n",
    "\n",
    "    # Decode the RLE-encoded mask\n",
    "    true_mask = rle_to_mask(rle_encoded_mask, (768, 768))  # replace with your function to decode the mask\n",
    "    true_mask = cv2.resize(true_mask, (256, 256))\n",
    "\n",
    "    # Add the true mask to the list\n",
    "    true_masks.append(true_mask)\n",
    "\n",
    "# convert to a binary mask\n",
    "binary_pred_masks = np.where(np.concatenate(pred_masks) > 0.5, 1, 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "iou_score = jaccard_score(np.concatenate(true_masks).ravel(), binary_pred_masks.ravel())\n",
    "\n",
    "print(f\"IOU Score: {iou_score}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dice = dice_score(np.concatenate(true_masks),  binary_pred_masks.ravel())\n",
    "\n",
    "print(f\"Dice Score: {dice}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Well the resuts are (again you can check):\n",
    "- IOU Score - 18%\n",
    "- Dice Score - 30%\n",
    "\n",
    "It`s kinda better if take into account that it was trained just on 6 epoches as I had no time to learn it more. But lets look where are the main error again via sanity check."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select a random image ID\n",
    "random_id = np.random.choice(test_image_ids)\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(os.path.join(image_dir, random_id))\n",
    "\n",
    "# Preprocess the image\n",
    "preprocessed_image = cv2.resize(image, (256, 256))  # resize to match the model's expected input size\n",
    "preprocessed_image = preprocess_input(preprocessed_image)\n",
    "\n",
    "# Reshape the image\n",
    "preprocessed_image = np.expand_dims(preprocessed_image, axis=0)\n",
    "\n",
    "# Make the prediction\n",
    "prediction = model.predict(preprocessed_image)[0]\n",
    "\n",
    "# Get the true mask for this image\n",
    "rle_encoded_mask = grouped.loc[random_id]\n",
    "\n",
    "# Decode the RLE-encoded mask\n",
    "true_mask = rle_to_mask(rle_encoded_mask, (768, 768))  # replace with your function to decode the mask\n",
    "true_mask = cv2.resize(true_mask, (256, 256))\n",
    "\n",
    "# Plot the image, the actual mask, and the predicted mask\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 20))\n",
    "\n",
    "axs[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "axs[0].set_title('Image')\n",
    "\n",
    "axs[1].imshow(true_mask, cmap='gray')\n",
    "axs[1].set_title('Actual Mask')\n",
    "\n",
    "axs[2].imshow(prediction.squeeze(), cmap='gray')\n",
    "axs[2].set_title('Predicted Mask')\n",
    "\n",
    "for ax in axs:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Well I was testing making these test when it was trained just with a 2 epoches and what can i say that it is reducing the error. Like there were a lot of problems with a clouds now these area that it predicts because of clouds is much smaller and in some cases it does not make this error. To my mind we can mitigate these problem by using the combination of classification and segmentation models.\n",
    " Also, when it predicts the shape becomes more look like a rectangle.\n",
    " There are error because of small ships and I think it because it was trained on the crops but now for testing we use rescaled images, that why it is difficult for him to predict them. So I should to add such an images to the training set or use the zoom augmentations."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "noteable-chatgpt": {
   "create_notebook": {
    "openai_conversation_id": "c1e65b6a-41f1-5cf4-8a96-81bfaacad508",
    "openai_ephemeral_user_id": "b546fc57-98f5-51e0-a3ac-6dcff4f4077a",
    "openai_subdivision1_iso_code": "UA-12"
   }
  },
  "noteable": {
   "last_transaction_id": "04cde956-f658-44f0-a7ee-37ab702d40af",
   "last_delta_id": "ebbb9ba1-abd3-4686-a9d6-2d5460b140f1"
  },
  "nteract": {
   "version": "noteable@2.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
